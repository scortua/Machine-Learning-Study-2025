{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24988795",
   "metadata": {},
   "source": [
    "# Arboles de decision\n",
    "Los árboles de decisión son modelos de aprendizaje supervisado utilizados tanto para clasificación como para regresión. Su estructura se asemeja a un árbol, donde cada nodo interno representa una prueba sobre un atributo, cada rama representa el resultado de la prueba y cada hoja representa una clase o valor de predicción.\n",
    "\n",
    "El algoritmo básico para construir un árbol de decisión consiste en seleccionar el atributo que mejor separa los datos según una medida de pureza (como la entropía o el índice de Gini), dividir el conjunto de datos en subconjuntos basados en ese atributo y repetir el proceso recursivamente para cada subconjunto. El proceso termina cuando los datos en un nodo son suficientemente puros o no se pueden realizar más divisiones.\n",
    "\n",
    "Los árboles de decisión son fáciles de interpretar y visualizar, pero pueden ser propensos al sobreajuste si no se podan adecuadamente.\n",
    "\n",
    "* Aprendizaje Supervisado\n",
    "* Aprendizaje basado en modelos\n",
    "* Son clasificadores no lineales. Al contrario de la regresion logistica o algoritmo SVM , no construye los limites de decision con lineas y planos rectos\n",
    "* Permiten la prediccion de valores continuos (regresion) y valores discretos (clasificacion)\n",
    "* Determina si un vector determinado pertenece a una clase o a otra estableciendo un conjunto de reglas de decision \"if-then else\" que terminan en la prediccion de una clase concreta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a54734",
   "metadata": {},
   "source": [
    "## Qué es un arbol de decision\n",
    "$\n",
    "(x_1^1,x_2^1,y^1),...,(x_1^m,x_2^m,y^m)\\\\\n",
    "y = {0,1,2}\n",
    "$\n",
    "Se cuentan la cantidad de ejemplos, despues si se dividen por la categoria se tiene una clasificacion y con ello armar un modelo que clasifique los limites. Para lo que si yo divido en dos subconjuntos, un conjunto tendra todos los de un subconjunto y el otro el de los otros 2 subconjuntos, entonces claramente no se puede aun clasificar y por lo tanto se sigue subdividiendo para quedar subconjuntos por clases unicas.\n",
    "\n",
    "![arbol ejemplo](Images/arboleje.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01067fb",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "El problema del estilo que vimos anteriormente es que entra en una clasificacion cerrada que puede sesgar el entrenamiento.\n",
    "\n",
    "La impureza de Gini es una métrica utilizada para medir la pureza de un conjunto de datos en los nodos de un árbol de decisión. Se calcula como la probabilidad de clasificar incorrectamente un elemento seleccionado al azar si se asigna una etiqueta según la distribución de clases en el nodo.\n",
    "\n",
    "Matemáticamente, para un nodo con $K$ clases, la impureza de Gini se define como:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "donde $p_k$ es la proporción de elementos de la clase $k$ en el nodo. Un valor de Gini bajo indica que el nodo es más puro (predominan los elementos de una sola clase), mientras que un valor alto indica mayor mezcla de clases. El algoritmo busca divisiones que minimicen la impureza de Gini en los nodos hijos.\n",
    "\n",
    "![Gini ejemplo](Images/ginieje.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba47dca",
   "metadata": {},
   "source": [
    "### ARBOL DECISION | ENTRENAMIENTO ALGORITMO\n",
    "#### *ENTRENAMIENTO I*\n",
    "1. **Selección de la característica (split variable):**  \n",
    "    * Se examinan todas las variables independientes del conjunto de datos $(x_1, x_2, ..., x_n)$ para identificar cuál de ellas permite separar mejor las clases objetivo $(y)$.  \n",
    "    * La selección se basa en una métrica de pureza, como la impureza de Gini, que cuantifica qué tan homogéneos son los grupos resultantes tras una posible división.\n",
    "\n",
    "2. **Determinación del valor de división (split value):**  \n",
    "    * Para la característica seleccionada, se consideran todos los valores posibles como candidatos para dividir el conjunto de datos.  \n",
    "    * El valor óptimo de división es aquel que genera los subconjuntos más puros, es decir, que maximiza la separación entre las clases.\n",
    "\n",
    "3. **Creación de nodos hijos:**  \n",
    "    * El conjunto de datos se divide en dos subconjuntos según el valor de división elegido, creando dos nodos hijos en el árbol.  \n",
    "    * Cada nodo hijo representa una rama que contiene los datos que cumplen o no cumplen la condición de la división.\n",
    "\n",
    "4. **Evaluación de la división con la métrica de pureza:**  \n",
    "    * Se calcula la impureza de Gini (u otra métrica) en cada nodo hijo para medir la calidad de la división.  \n",
    "    * Una división efectiva produce nodos con baja impureza, es decir, con predominancia de una sola clase.\n",
    "\n",
    "5. **Repetición del proceso:**  \n",
    "    * Si algún nodo hijo aún contiene una mezcla significativa de clases, se repite el proceso de selección de característica y valor de división sobre ese nodo.  \n",
    "    * El procedimiento es recursivo y continúa hasta que los nodos sean suficientemente puros o se alcance un criterio de parada (como un número mínimo de muestras o profundidad máxima).\n",
    "\n",
    "#### *ENTRENAMIENTO II*\n",
    "1. **Exploración de combinaciones:**  \n",
    "    * El árbol analiza todas las posibles combinaciones de características y valores de división en cada nodo, buscando la opción que genere los subconjuntos más puros según la métrica seleccionada (por ejemplo, impureza de Gini).\n",
    "\n",
    "2. **Construcción recursiva del árbol:**  \n",
    "    * Para cada nodo hijo creado, se repite el proceso de selección de característica y valor de división, profundizando en el árbol de manera recursiva.  \n",
    "    * Este procedimiento continúa hasta que se cumple algún criterio de parada, como alcanzar nodos completamente puros, un número mínimo de muestras por nodo, o una profundidad máxima del árbol.\n",
    "\n",
    "3. **Poda del árbol (opcional):**  \n",
    "    * Una vez construido el árbol completo, se pueden eliminar (podar) ramas que no aportan valor predictivo significativo.  \n",
    "    * La poda ayuda a reducir la complejidad del modelo y a evitar el sobreajuste, manteniendo únicamente las divisiones que mejoran la capacidad de generalización del árbol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a959f",
   "metadata": {},
   "source": [
    "## Classification and Regression Tree (CART)\n",
    "El algoritmo CART (Classification and Regression Tree) es una técnica fundamental para construir árboles de decisión tanto en tareas de clasificación como de regresión.\n",
    "\n",
    "- **Clasificación:** En problemas de clasificación, el árbol divide el espacio de características en regiones homogéneas respecto a la variable objetivo (clase). Cada hoja del árbol representa una clase y la predicción se realiza asignando la clase mayoritaria de los ejemplos que caen en esa hoja.\n",
    "\n",
    "- **Regresión:** En problemas de regresión, el árbol divide el espacio de características en regiones donde la variable objetivo es continua. Cada hoja del árbol predice el valor promedio de los ejemplos que llegan a esa hoja.\n",
    "\n",
    "CART utiliza métricas como la impureza de Gini para clasificación y el error cuadrático medio para regresión, buscando en cada división maximizar la pureza o minimizar el error. El resultado es un modelo interpretable que puede visualizarse como un conjunto de reglas de decisión secuenciales.\n",
    "\n",
    "Recordando la ecuacion de gini:\n",
    "$$\n",
    "Gini = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "Con lo que la funcion de costo es:\n",
    "$$\n",
    "J(k,t_k)=\\frac{m_izq}{m_total}*G_izq + \\frac{m_der}{m_total}*G_der\n",
    "$$\n",
    "\n",
    "m es el numero de ejemplos de datos de entrenamiento. Entonces lo que hace es una suma ponderada.\n",
    "\n",
    "Los arboles de decision son capaces de hacer tareas de clasificacion, pero tambien de regresion.\n",
    "\n",
    "$$\n",
    "MSE = \\sum_{i=1}^{n_{node}} (\\hat{y}_{node}-y^{i})^2\\\\\n",
    "\\hat{y}_{node} = \\frac{1}{m_{node}} \\sum_{i=0}^{m_{node}} y^{i}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93f2b5",
   "metadata": {},
   "source": [
    "## Limitaciones de los arboles de decision\n",
    "* La versión implementada en Scikit-learn puede producir árboles de decisión complejos que producen `Overfitting` sobre el conjunto de datos\n",
    "* La implementación del algoritmo, se basa en la `obtención de óptimos locales` entre el nodo padre y el nodo hijo. Como consecuencia, no hay manera de asegurar que el árbol a alcance una forma óptima global\n",
    "* Los árboles de decisión `pueden ser inestables`. Una pequeña variación en el conjunto de datos puede producir que se genere un árbol completamente diferente\n",
    "* Pueden `mitigarse gran parte de los efectos` de estas limitaciones si se utilizan conjuntos aleatorios de árboles que toman decisiones conjuntas `(Random Forest)`\n",
    "\n",
    "### Overfiting\n",
    "Los árboles de decisión también pueden producir Overfitting. Cuando existe un número elevado de características o de valores atípicos, es posible que el árbol de decisión genere un número muy elevado de ramas que desemboque en problemas para generalizar\n",
    "* Aplicar un criterio de parada (hiperparámetros):\n",
    "    * **max_depth**. Determina la cantidad máxima de ramas y hojas permitidas antes de que termine la construcción del árbol.\n",
    "    * **min_samples_split**. Determina el número mínimo de muestras que un nodo debe poseer para ser elegible para una división.\n",
    "    * **min_samples_leaf**. Determina el número mínimo de muestras necesarias para crear una hoja secundaria.\n",
    "    * **max_leaf_nodes**. Determina cuántas hojas se pueden crear en total\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[#30 15,15] --> B[#15 14,1]\n",
    "    A --> C[#15 1,14]\n",
    "    B --> D[#14 14,0]\n",
    "    B --> E[#14 0,14]\n",
    "    D --> F[Clase 0]\n",
    "    E --> G[Clase 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d591f",
   "metadata": {},
   "source": [
    "# Ensamble Learning y Random Forest\n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "El aprendizaje por ensamble (Ensemble Learning) es una técnica que combina múltiples modelos para mejorar el rendimiento predictivo. En lugar de confiar en un solo modelo, se utilizan varios modelos para obtener una predicción más robusta y precisa. Las principales ventajas del ensemble learning incluyen:\n",
    "\n",
    "- Reduce el sobreajuste (overfitting)\n",
    "- Mejora la precisión y estabilidad de las predicciones\n",
    "- Captura relaciones complejas que un solo modelo podría no detectar\n",
    "\n",
    "Los métodos principales de ensemble learning son:\n",
    "\n",
    "1. **Bagging**: Entrena varios modelos en diferentes subconjuntos de datos (muestreados con reemplazo) y promedia sus predicciones\n",
    "2. **Boosting**: Entrena modelos secuencialmente, cada uno enfocándose en los errores del anterior\n",
    "3. **Stacking**: Combina las predicciones de varios modelos usando otro modelo\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Random Forest es un algoritmo de ensemble learning basado en la técnica de bagging, que utiliza múltiples árboles de decisión para mejorar la precisión predictiva y controlar el sobreajuste.\n",
    "\n",
    "### Características clave:\n",
    "\n",
    "- **Construcción de múltiples árboles**: Crea un \"bosque\" de árboles de decisión, cada uno entrenado con un subconjunto aleatorio de datos\n",
    "- **Selección aleatoria de características**: En cada nodo, considera solo un subconjunto aleatorio de características para la división\n",
    "- **Votación mayoritaria**: Para clasificación, la clase más votada entre todos los árboles es la predicción final\n",
    "- **Promedio**: Para regresión, el resultado es el promedio de las predicciones de todos los árboles\n",
    "\n",
    "### Ventajas:\n",
    "\n",
    "- Alta precisión incluso con datos de alta dimensionalidad\n",
    "- Estima la importancia de cada variable en la clasificación\n",
    "- Maneja eficientemente grandes conjuntos de datos\n",
    "- Robusto frente a outliers y ruido\n",
    "- Reduce significativamente el riesgo de overfitting\n",
    "\n",
    "### Hiperparámetros importantes:\n",
    "\n",
    "- `n_estimators`: Número de árboles en el bosque\n",
    "- `max_features`: Número máximo de características consideradas en cada división\n",
    "- `max_depth`: Profundidad máxima de los árboles\n",
    "- `min_samples_split`: Mínimo de muestras para dividir un nodo\n",
    "- `bootstrap`: Si usar o no muestreo con reemplazo\n",
    "\n",
    "Random Forest representa una solución eficaz a las limitaciones de los árboles de decisión individuales, ofreciendo mayor estabilidad y capacidad de generalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a5f4f",
   "metadata": {},
   "source": [
    "# Ensamble Learning: Bagging y Pasting\n",
    "\n",
    "## Bagging y Pasting: Métodos de Ensemble Learning\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "Bagging es una técnica de ensemble learning que crea múltiples versiones del mismo modelo, entrenando cada uno con diferentes subconjuntos del conjunto de datos original, muestreados con reemplazo (bootstrap). Características principales:\n",
    "\n",
    "- **Muestreo con reemplazo**: Cada subconjunto puede contener ejemplos duplicados\n",
    "- **Entrenamiento paralelo**: Los modelos se entrenan independientemente\n",
    "- **Agregación**: Las predicciones se combinan por votación (clasificación) o promedio (regresión)\n",
    "- **Reduce la varianza**: Disminuye el riesgo de sobreajuste al promediar predicciones de múltiples modelos\n",
    "\n",
    "### Pasting\n",
    "Similar al bagging, pero el muestreo se realiza sin reemplazo. Cada subconjunto contiene ejemplos únicos del conjunto de datos original. Diferencias clave:\n",
    "\n",
    "- **Muestreo sin reemplazo**: No hay duplicados en los subconjuntos\n",
    "- **Mayor diversidad en los datos**: Cada modelo ve una parte diferente del conjunto de datos\n",
    "- **Menor correlación entre modelos**: Potencialmente mejor cuando los datos contienen ruido\n",
    "\n",
    "### Comparación\n",
    "- Bagging suele funcionar mejor cuando los modelos base son complejos y tienen alta varianza\n",
    "- Pasting puede ser preferible cuando se quiere minimizar el impacto de valores atípicos\n",
    "- Ambos métodos se benefician de la paralelización, ya que los modelos son independientes\n",
    "\n",
    "### Aplicación en Random Forest\n",
    "Random Forest es una implementación de bagging aplicada a árboles de decisión, con la adición de selección aleatoria de características en cada nodo, lo que añade diversidad adicional entre los modelos del ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfceffe",
   "metadata": {},
   "source": [
    "# Ensamble Learning: Random Forest\n",
    "\n",
    "## Random Forest: Un poderoso método de ensemble learning\n",
    "\n",
    "Random Forest es una técnica avanzada de ensemble learning que extiende el concepto de bagging aplicado específicamente a los árboles de decisión. Desarrollado por Leo Breiman en 2001, combina las predicciones de múltiples árboles de decisión para producir un modelo más preciso y estable.\n",
    "\n",
    "### Funcionamiento del algoritmo\n",
    "\n",
    "1. **Muestreo con reemplazo (bootstrap)**: Para cada árbol, se selecciona una muestra aleatoria del conjunto de datos original con reemplazo\n",
    "   \n",
    "2. **Selección aleatoria de características**: En cada nodo del árbol, solo se considera un subconjunto aleatorio de características para realizar la división\n",
    "   \n",
    "3. **Construcción completa de árboles**: Los árboles se desarrollan al máximo sin poda\n",
    "   \n",
    "4. **Agregación de resultados**: \n",
    "   - En clasificación: se utiliza votación mayoritaria\n",
    "   - En regresión: se promedian las predicciones de todos los árboles\n",
    "\n",
    "### Ventajas principales\n",
    "\n",
    "- **Alta precisión**: Generalmente supera a los árboles individuales y muchos otros algoritmos\n",
    "- **Robustez ante el ruido y los outliers**: El promediado reduce el impacto de datos anómalos\n",
    "- **Estimación incorporada de importancia de variables**: Permite identificar qué características son más relevantes\n",
    "- **Manejo efectivo de datos de alta dimensionalidad**\n",
    "- **Capacidad para modelar interacciones complejas entre variables**\n",
    "- **Menor tendencia al sobreajuste** en comparación con árboles individuales\n",
    "\n",
    "### Consideraciones clave\n",
    "\n",
    "- **Número de árboles (n_estimators)**: Más árboles generalmente mejoran el rendimiento hasta cierto punto\n",
    "- **Profundidad de los árboles**: Árboles más profundos capturan relaciones más complejas pero pueden aumentar el riesgo de overfitting\n",
    "- **Número de características consideradas (max_features)**: Afecta la diversidad entre los árboles\n",
    "- **Tamaño mínimo de nodos (min_samples_split/min_samples_leaf)**: Controla el nivel de detalle del modelo\n",
    "\n",
    "### Out-of-Bag (OOB) Evaluation\n",
    "\n",
    "Una ventaja única de Random Forest es la evaluación OOB: como cada árbol se entrena con aproximadamente 2/3 de las muestras (debido al muestreo bootstrap), el tercio restante puede usarse como conjunto de validación natural para cada árbol, proporcionando una estimación interna del error de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47867e",
   "metadata": {},
   "source": [
    "# Ensamble Learning: Boosting y Stacking\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting es una técnica de ensemble learning secuencial donde cada modelo se entrena para corregir los errores de los modelos anteriores. A diferencia del bagging, los modelos se construyen de manera secuencial y cada nuevo modelo se enfoca en los ejemplos que fueron mal clasificados por los modelos anteriores.\n",
    "\n",
    "### Características principales:\n",
    "- **Aprendizaje secuencial**: Cada modelo aprende de los errores del anterior\n",
    "- **Ponderación**: Se asignan pesos mayores a las observaciones mal clasificadas\n",
    "- **Modelos débiles**: Utiliza \"weak learners\" (modelos simples) que juntos forman un modelo fuerte\n",
    "- **Reduce el sesgo**: Efectivo para disminuir tanto el bias como la varianza\n",
    "\n",
    "### Algoritmos populares:\n",
    "1. **AdaBoost**: Ajusta los pesos de las observaciones según la precisión de clasificación\n",
    "2. **Gradient Boosting**: Optimiza una función de pérdida mediante descenso de gradiente\n",
    "3. **XGBoost**: Implementación optimizada de gradient boosting con regularización\n",
    "4. **LightGBM**: Algoritmo de Microsoft optimizado para eficiencia y velocidad\n",
    "5. **CatBoost**: Manejo eficiente de variables categóricas y menos overfitting\n",
    "\n",
    "## Stacking\n",
    "\n",
    "Stacking (o Stacked Generalization) es un método de ensemble learning que combina múltiples modelos de clasificación o regresión mediante un meta-modelo que aprende cómo integrar mejor las predicciones de los modelos base.\n",
    "\n",
    "### Funcionamiento:\n",
    "1. **Modelos base**: Se entrenan varios modelos heterogéneos (de diferentes tipos) en el mismo conjunto de datos\n",
    "2. **Generación de meta-características**: Las predicciones de los modelos base se utilizan como características para el meta-modelo\n",
    "3. **Meta-modelo**: Un modelo final que aprende la mejor manera de combinar las predicciones de los modelos base\n",
    "4. **Validación cruzada**: Las predicciones para el meta-modelo suelen generarse mediante validación cruzada para evitar fugas de información\n",
    "\n",
    "### Ventajas:\n",
    "- **Aprovecha las fortalezas de diferentes algoritmos**\n",
    "- **Mayor poder predictivo** que cualquiera de los modelos individuales\n",
    "- **Flexibilidad**: Permite combinar cualquier tipo de modelo\n",
    "- **Captura relaciones complejas** entre los patrones detectados por diferentes modelos\n",
    "\n",
    "### Consideraciones:\n",
    "- Requiere mayor tiempo de computación y complejidad\n",
    "- Riesgo de sobreajuste si no se implementa correctamente\n",
    "- La selección adecuada de los modelos base y el meta-modelo es crítica\n",
    "\n",
    "El stacking es particularmente efectivo cuando los modelos base tienen diferentes fortalezas y debilidades, permitiendo que el meta-modelo aprenda cuándo confiar en cada uno de ellos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBMMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
