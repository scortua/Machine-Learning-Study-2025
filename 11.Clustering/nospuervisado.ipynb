{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18ce26c",
   "metadata": {},
   "source": [
    "# Algoritmos no supervisados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198e0fb",
   "metadata": {},
   "source": [
    "## Que es el clustering\n",
    "\n",
    "El clustering es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos llamados clusters. A diferencia de los métodos supervisados, no requiere datos etiquetados previamente y busca encontrar patrones o estructuras inherentes en los datos.\n",
    "\n",
    "Las características principales del clustering incluyen:\n",
    "\n",
    "- **Agrupación por similitud**: Los objetos dentro de un mismo cluster son más similares entre sí que con objetos de otros clusters\n",
    "- **Descubrimiento de patrones**: Permite identificar estructuras naturales en los datos sin conocimiento previo\n",
    "- **Aplicaciones diversas**: Se utiliza en segmentación de clientes, análisis de imágenes, bioinformática y reconocimiento de patrones\n",
    "\n",
    "Los algoritmos más comunes incluyen:\n",
    "- K-means: Divide los datos en k grupos minimizando la varianza intra-cluster\n",
    "- Clustering jerárquico: Construye una jerarquía de clusters mediante divisiones o fusiones sucesivas\n",
    "- DBSCAN: Agrupa puntos basándose en la densidad, identificando regiones de alta densidad separadas por regiones de baja densidad\n",
    "\n",
    "El clustering es fundamental en la exploración de datos y sirve como base para muchas tareas de análisis más avanzadas.\n",
    "\n",
    "* Objetivo: Agrupar de manera coherente un conjunto de datos sin etiquetar en subconjuntos o clusters\n",
    "* Agrupación de los datos mediante el *concepto de proximidad* entre ellos\n",
    "    * Métrica: método concreto con el que se evalúa la cercanía entre los puntos\n",
    "        * Ejemplo rudimentario de clustering: Escoger una o varias dimensiones y definir cada cluster como el conjunto de elementos que comparten valores en esas dimensiones.\n",
    "        * Ejemplo: Si eliges la dirección IP, se define un cluster por cada dirección IP (GROUP BY de SQL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d348b",
   "metadata": {},
   "source": [
    "## Tecnicas de machine learning que usan clustering\n",
    "\n",
    "El clustering se integra en múltiples técnicas avanzadas de machine learning:\n",
    "\n",
    "- **Reducción de dimensionalidad**: Algoritmos como t-SNE y UMAP utilizan conceptos de clustering para visualizar datos de alta dimensionalidad\n",
    "- **Sistemas de recomendación**: Agrupan usuarios o productos con comportamientos similares para generar recomendaciones\n",
    "- **Detección de anomalías**: Identifican outliers como puntos que no pertenecen claramente a ningún cluster\n",
    "- **Aprendizaje semi-supervisado**: Utilizan clusters para generar pseudo-etiquetas cuando los datos etiquetados son escasos\n",
    "- **Segmentación de imágenes**: Agrupan píxeles similares para identificar objetos o regiones\n",
    "- **Procesamiento de lenguaje natural**: Clustering de documentos o palabras para descubrir temas o similitudes semánticas\n",
    "- **Análisis de series temporales**: Agrupan patrones similares en datos secuenciales para identificar comportamientos recurrentes\n",
    "\n",
    "Estas técnicas aprovechan la capacidad del clustering para descubrir estructuras subyacentes en los datos, potenciando así otros algoritmos y metodologías de análisis más complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e3708",
   "metadata": {},
   "source": [
    "### Kmeans\n",
    "\n",
    "K-means es uno de los algoritmos de clustering más populares y ampliamente utilizados debido a su simplicidad y eficiencia. Este método particiona un conjunto de datos en K clusters distintos, donde cada punto pertenece al cluster con el centroide más cercano.\n",
    "\n",
    "#### Funcionamiento del algoritmo:\n",
    "\n",
    "1. **Inicialización**: Se seleccionan K puntos como centroides iniciales (aleatoriamente o mediante métodos como K-means++)\n",
    "2. **Asignación**: Cada punto del conjunto de datos se asigna al centroide más cercano, formando K clusters\n",
    "3. **Actualización**: Se recalculan los centroides como la media de todos los puntos asignados a cada cluster\n",
    "4. **Iteración**: Se repiten los pasos 2 y 3 hasta que los centroides se estabilicen o se alcance un número máximo de iteraciones\n",
    "\n",
    "#### Características principales:\n",
    "\n",
    "- **Convergencia garantizada**: Siempre converge a un mínimo local de la función objetivo\n",
    "- **Complejidad lineal**: O(n·K·d·i) donde n es el número de puntos, K el número de clusters, d la dimensionalidad y i el número de iteraciones\n",
    "- **Requiere especificar K**: El usuario debe definir el número de clusters a priori\n",
    "- **Sensible a outliers**: Los valores atípicos pueden afectar significativamente la posición de los centroides\n",
    "- **Asume clusters convexos**: Funciona mejor con clusters esféricos de tamaño similar\n",
    "\n",
    "#### Limitaciones:\n",
    "\n",
    "- No garantiza encontrar la solución óptima global\n",
    "- Los resultados dependen de la inicialización de los centroides\n",
    "- Dificultad para manejar clusters de formas irregulares o densidades variables\n",
    "- No adecuado para datos categóricos sin una adaptación específica\n",
    "\n",
    "A pesar de sus limitaciones, K-means sigue siendo fundamental en muchas aplicaciones debido a su escalabilidad y facilidad de implementación, especialmente en fases exploratorias del análisis de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb9c8c",
   "metadata": {},
   "source": [
    "# Ejemplo Matemático de Centroides y Distancias en K-means\n",
    "\n",
    "## Representación Matemática\n",
    "\n",
    "Sean los puntos $X = \\{x_1, x_2, ..., x_n\\}$ donde cada $x_i \\in \\mathbb{R}^d$ es un vector de $d$ dimensiones.\n",
    "\n",
    "### Centroides\n",
    "\n",
    "Para $k$ clusters, los centroides se representan como $C = \\{c_1, c_2, ..., c_k\\}$ donde cada $c_j \\in \\mathbb{R}^d$.\n",
    "\n",
    "El centroide $c_j$ del cluster $j$ se calcula como la media aritmética de todos los puntos asignados a ese cluster:\n",
    "\n",
    "$$c_j = \\frac{1}{|S_j|} \\sum_{x_i \\in S_j} x_i$$\n",
    "\n",
    "donde $S_j$ es el conjunto de puntos asignados al cluster $j$.\n",
    "\n",
    "### Distancia Euclidiana\n",
    "\n",
    "La distancia euclidiana entre un punto $x_i$ y un centroide $c_j$ se calcula como:\n",
    "\n",
    "$$d(x_i, c_j) = \\sqrt{\\sum_{l=1}^{d} (x_{i,l} - c_{j,l})^2}$$\n",
    "\n",
    "### Función Objetivo\n",
    "\n",
    "K-means busca minimizar la suma de las distancias al cuadrado entre cada punto y su centroide más cercano:\n",
    "\n",
    "$$J = \\sum_{j=1}^{k} \\sum_{x_i \\in S_j} ||x_i - c_j||^2$$\n",
    "\n",
    "## Ejemplo Numérico\n",
    "\n",
    "Considerando puntos en 2D:\n",
    "- $x_1 = (2, 3)$\n",
    "- $x_2 = (3, 4)$\n",
    "- $x_3 = (8, 7)$\n",
    "- $x_4 = (9, 8)$\n",
    "\n",
    "Con $k=2$ y centroides iniciales $c_1 = (2, 3)$ y $c_2 = (8, 7)$.\n",
    "\n",
    "La asignación inicial sería:\n",
    "- Cluster 1: $\\{x_1, x_2\\}$\n",
    "- Cluster 2: $\\{x_3, x_4\\}$\n",
    "\n",
    "Nuevos centroides después de una iteración:\n",
    "- $c_1 = \\frac{(2,3) + (3,4)}{2} = (2.5, 3.5)$\n",
    "- $c_2 = \\frac{(8,7) + (9,8)}{2} = (8.5, 7.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbdd86",
   "metadata": {},
   "source": [
    "# Algoritmo K-means\n",
    "\n",
    "```\n",
    "función k_means(k, dataset):\n",
    "    # Inicialización aleatoria de los centroides\n",
    "    centroides = seleccionar_k_puntos_aleatorios(dataset, k)\n",
    "    \n",
    "    # Vector para almacenar la asignación de cada punto a un cluster\n",
    "    asignaciones = vector_vacío(longitud(dataset))\n",
    "    \n",
    "    repetir:\n",
    "        # Asignación de cada punto al centroide más cercano\n",
    "        para i = 1 hasta longitud(dataset):\n",
    "            asignaciones[i] = índice_del_centroide_más_cercano(dataset[i], centroides)\n",
    "        \n",
    "        # Actualización de los centroides\n",
    "        centroides_antiguos = centroides\n",
    "        para j = 1 hasta k:\n",
    "            puntos_del_cluster = {dataset[i] | asignaciones[i] == j}\n",
    "            centroides[j] = calcular_media(puntos_del_cluster)\n",
    "            \n",
    "    hasta que centroides == centroides_antiguos o alcance_max_iteraciones\n",
    "    \n",
    "    retornar centroides, asignaciones\n",
    "```\n",
    "\n",
    "El algoritmo k-means divide los datos en k grupos minimizando la suma de distancias cuadráticas entre cada punto y el centroide de su cluster asignado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae5de1",
   "metadata": {},
   "source": [
    "# Limitaciones del algoritmo K-means\n",
    "\n",
    "El algoritmo K-means, a pesar de su popularidad, presenta varias limitaciones importantes:\n",
    "\n",
    "- **Selección de K**: Requiere especificar el número de clusters a priori, lo que puede ser difícil sin conocimiento previo del dataset\n",
    "- **Sensibilidad a la inicialización**: Los resultados varían según los centroides iniciales, pudiendo converger a mínimos locales\n",
    "- **Limitado a formas convexas**: Asume clusters esféricos y de tamaño similar, fallando con formas irregulares\n",
    "- **Afectado por outliers**: Los valores atípicos distorsionan significativamente la posición de los centroides\n",
    "- **No determinístico**: Diferentes ejecuciones pueden producir resultados distintos\n",
    "- **Escalas de características**: Sensible a las diferentes escalas de las variables\n",
    "- **Limitaciones con datos categóricos**: Diseñado principalmente para variables numéricas continuas\n",
    "- **Densidad variable**: No maneja bien clusters con diferentes densidades\n",
    "- **Convergencia lenta**: En conjuntos grandes o de alta dimensionalidad, puede requerir muchas iteraciones\n",
    "\n",
    "* Se debe intuir el número de clusters que genera el algoritmo. Si son datos etiquetados, elegir el número de clusters como un valor entre uno y tres veces el número de etiquetas existentes\n",
    "* Hay que aplicar normalización al conjunto de datos\n",
    "* No debe utilizarse KMEANS con datos categóricos a los que se le aplica one-hot encoding.\n",
    "* Por el contrario, deben tratar de codificarse estas características como multiple binary\n",
    "* Pierde eficiencia en conjuntos de datos con muchas dimensiones. Una practica frecuente es reducir las dimensiones del conjunto de datos mediante el uso de PCA o SVD\n",
    "* Funciona mejor si los centroides iniciales se eligen aleatoriamente. Esto provoca que los resultados puedan cambiar dependiendo de donde se inicialicen\n",
    "* Asume que los clusters son esféricos. No funciona correctamente en distribuciones de datos no esféricas.\n",
    "\n",
    "Para abordar estas limitaciones, existen variaciones como K-means++ (mejora la inicialización), K-medoids (más robusto a outliers), o algoritmos completamente diferentes como DBSCAN o clustering jerárquico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e8b50",
   "metadata": {},
   "source": [
    "# DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "DBSCAN es un algoritmo de clustering basado en densidad que identifica grupos de puntos densamente agrupados separados por regiones de baja densidad.\n",
    "\n",
    "## Características principales\n",
    "\n",
    "- **No requiere especificar el número de clusters**: Descubre automáticamente la cantidad de grupos en los datos\n",
    "- **Identifica ruido**: Clasifica explícitamente puntos como \"ruido\" si no pertenecen a ningún cluster denso\n",
    "- **Detecta clusters de forma arbitraria**: No está limitado a formas esféricas como K-means\n",
    "- **Basado en dos parámetros clave**:\n",
    "    - **Epsilon (ε)**: Radio que define el vecindario de un punto\n",
    "    - **MinPts**: Número mínimo de puntos necesarios en el vecindario para formar un core point\n",
    "\n",
    "## Funcionamiento del algoritmo\n",
    "\n",
    "1. **Clasificación de puntos**:\n",
    "     - **Core point**: Punto con al menos MinPts puntos en su vecindario de radio ε\n",
    "     - **Border point**: Punto en el vecindario de un core point pero con menos de MinPts vecinos\n",
    "     - **Noise point**: Punto que no es ni core ni border\n",
    "\n",
    "2. **Formación de clusters**:\n",
    "     - Conecta core points que son vecinos directamente\n",
    "     - Une core points mutuamente alcanzables a través de otros core points\n",
    "     - Asigna border points al cluster de sus core points vecinos\n",
    "\n",
    "![DBSCAN EXAMPLE](Images/DBSCAN.png)\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Identifica clusters de formas variadas\n",
    "- Detecta automáticamente outliers\n",
    "- No asume distribuciones específicas en los datos\n",
    "- No requiere conocer el número de clusters a priori\n",
    "\n",
    "## Limitaciones\n",
    "\n",
    "- Sensible a la elección de parámetros ε y MinPts\n",
    "- Dificultad con clusters de densidades muy variables\n",
    "- Problemas en espacios de alta dimensionalidad debido a la \"maldición de la dimensionalidad\"\n",
    "- Mayor complejidad computacional que K-means (O(n²) en el peor caso)\n",
    "\n",
    "DBSCAN es particularmente útil en aplicaciones donde los datos contienen ruido, los clusters tienen formas irregulares o cuando el número de clusters no es conocido de antemano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece23c63",
   "metadata": {},
   "source": [
    "# Ejemplo Matemático de DBSCAN: Densidad y Conectividad\n",
    "\n",
    "## Definiciones Matemáticas\n",
    "\n",
    "Sea un conjunto de datos $X = \\{x_1, x_2, ..., x_n\\}$ donde cada $x_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "### Conceptos Fundamentales\n",
    "\n",
    "1. **Vecindario-ε de un punto**:\n",
    "    El conjunto de puntos a una distancia menor o igual que $\\varepsilon$ de $x$:\n",
    "    $$N_\\varepsilon(x) = \\{y \\in X \\mid d(x,y) \\leq \\varepsilon\\}$$\n",
    "\n",
    "2. **Punto núcleo (Core point)**:\n",
    "    Un punto $p$ es núcleo si su vecindario contiene al menos MinPts puntos:\n",
    "    $$|N_\\varepsilon(p)| \\geq \\text{MinPts}$$\n",
    "\n",
    "3. **Alcance directo por densidad**:\n",
    "    Un punto $q$ es directamente alcanzable desde $p$ si:\n",
    "    $$p \\text{ es un punto núcleo y } q \\in N_\\varepsilon(p)$$\n",
    "\n",
    "4. **Alcance por densidad**:\n",
    "    Un punto $q$ es alcanzable por densidad desde $p$ si existe una cadena de puntos $p_1, p_2, ..., p_m$ con $p_1 = p$ y $p_m = q$ donde cada $p_{i+1}$ es directamente alcanzable desde $p_i$.\n",
    "\n",
    "5. **Conexión por densidad**:\n",
    "    Dos puntos $p$ y $q$ están conectados por densidad si existe un punto $o$ tal que ambos $p$ y $q$ son alcanzables por densidad desde $o$.\n",
    "\n",
    "## Ejemplo Numérico\n",
    "\n",
    "Consideremos puntos en un plano 2D con $\\varepsilon = 2$ y $\\text{MinPts} = 3$:\n",
    "\n",
    "- $x_1 = (1, 1)$\n",
    "- $x_2 = (2, 2)$\n",
    "- $x_3 = (1.5, 1.8)$\n",
    "- $x_4 = (8, 7)$\n",
    "- $x_5 = (8.5, 8)$\n",
    "- $x_6 = (4, 5)$\n",
    "\n",
    "### Análisis del Vecindario\n",
    "\n",
    "Para $x_1$:\n",
    "$$N_\\varepsilon(x_1) = \\{x_1, x_2, x_3\\}$$\n",
    "La distancia euclidiana entre $x_1$ y $x_2$ es:\n",
    "$$d(x_1, x_2) = \\sqrt{(2-1)^2 + (2-1)^2} = \\sqrt{2} \\approx 1.41 < 2$$\n",
    "\n",
    "Como $|N_\\varepsilon(x_1)| = 3 = \\text{MinPts}$, $x_1$ es un punto núcleo.\n",
    "\n",
    "De manera similar, $x_2$ y $x_3$ son puntos núcleo, mientras que $x_6$ es un punto de ruido ya que está aislado.\n",
    "\n",
    "### Formación de Clusters\n",
    "\n",
    "- Cluster 1: $\\{x_1, x_2, x_3\\}$ (todos conectados por densidad)\n",
    "- Cluster 2: $\\{x_4, x_5\\}$ (conectados entre sí)\n",
    "- Ruido: $\\{x_6\\}$ (no conectado a ningún otro punto dentro de $\\varepsilon$)\n",
    "\n",
    "Este ejemplo ilustra cómo DBSCAN identifica clusters basados en densidad y clasifica puntos aislados como ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c86de",
   "metadata": {},
   "source": [
    "# Evaluación de mecanismos de clustering\n",
    "\n",
    "La evaluación de algoritmos de clustering es desafiante ya que, al ser no supervisados, no hay etiquetas \"correctas\" para comparar. Sin embargo, existen varias métricas y enfoques para validar la calidad de los clusters:\n",
    "\n",
    "## Métricas internas (cuando no hay etiquetas)\n",
    "\n",
    "- **Coeficiente de Silhouette**: Mide qué tan similar es un punto a su propio cluster comparado con otros clusters (rango de -1 a 1)\n",
    "    - Valores cercanos a 1 indican buena separación\n",
    "    - Valores cercanos a 0 indican solapamiento\n",
    "    - Valores negativos sugieren asignaciones incorrectas\n",
    "\n",
    "- **Índice Davies-Bouldin**: Cuantifica la separación promedio entre clusters (valores más bajos son mejores)\n",
    "\n",
    "- **Inercia/SSE**: Suma de distancias cuadráticas de cada punto a su centroide\n",
    "    - Útil para el método \"elbow\" para determinar el número óptimo de clusters en K-means\n",
    "\n",
    "- **Índice Calinski-Harabasz**: Mide el ratio entre la dispersión entre clusters y la dispersión dentro de los clusters\n",
    "\n",
    "## Métricas externas (cuando hay etiquetas disponibles)\n",
    "\n",
    "- **Índice de Rand Ajustado (ARI)**: Mide la similitud entre dos asignaciones de clusters, ajustado para el azar\n",
    "    - Rango de -1 a 1, donde 1 es concordancia perfecta\n",
    "\n",
    "- **Información Mutua Normalizada (NMI)**: Cuantifica la información compartida entre la asignación de clusters y las etiquetas reales\n",
    "\n",
    "- **Homogeneidad y Completitud**: \n",
    "    - Homogeneidad: cada cluster contiene solo miembros de una misma clase\n",
    "    - Completitud: todos los miembros de una misma clase están en el mismo cluster\n",
    "\n",
    "## Validación visual\n",
    "\n",
    "- **Visualización de clusters**: Proyecciones 2D/3D usando PCA, t-SNE o UMAP\n",
    "- **Dendrogramas**: Para evaluar clustering jerárquico\n",
    "- **Mapas de calor de matrices de distancia**: Para examinar la estructura de similitud\n",
    "\n",
    "## Análisis de estabilidad\n",
    "\n",
    "- **Validación cruzada**: Comparar clusters obtenidos con diferentes submuestras de datos\n",
    "- **Perturbación de datos**: Evaluar cómo cambian los clusters al añadir ruido o variar parámetros\n",
    "\n",
    "La elección de la métrica depende del contexto específico del problema y de los objetivos del análisis de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be394b8",
   "metadata": {},
   "source": [
    "# Técnicas para Evaluar Clusters Especificados\n",
    "\n",
    "## Homogeneidad\n",
    "\n",
    "La homogeneidad mide si cada cluster contiene únicamente miembros de una sola clase. Un clustering tiene homogeneidad perfecta cuando todos los clusters contienen solo puntos de datos que pertenecen a una única clase.\n",
    "\n",
    "- **Fórmula**: $h = 1 - \\frac{H(C|K)}{H(C)}$\n",
    "    - Donde $H(C|K)$ es la entropía condicional de las clases dados los clusters\n",
    "    - $H(C)$ es la entropía de las clases\n",
    "\n",
    "- **Rango**: Entre 0 y 1, donde 1 indica perfecta homogeneidad\n",
    "- **Interpretación**: Un valor alto significa que los puntos dentro de cada cluster pertenecen principalmente a la misma clase\n",
    "\n",
    "## Completitud (Plenitud)\n",
    "\n",
    "La completitud evalúa si todos los puntos de datos que pertenecen a una misma clase se encuentran en un único cluster. Es perfecta cuando todos los miembros de una clase están asignados al mismo cluster.\n",
    "\n",
    "- **Fórmula**: $c = 1 - \\frac{H(K|C)}{H(K)}$\n",
    "    - Donde $H(K|C)$ es la entropía condicional de los clusters dadas las clases\n",
    "    - $H(K)$ es la entropía de los clusters\n",
    "\n",
    "- **Rango**: Entre 0 y 1, donde 1 indica perfecta completitud\n",
    "- **Interpretación**: Un valor alto indica que los miembros de una clase tienden a estar asignados al mismo cluster\n",
    "\n",
    "## V-Measure\n",
    "\n",
    "V-Measure es la media armónica entre homogeneidad y completitud, similar al F1-score en clasificación.\n",
    "\n",
    "- **Fórmula**: $V_\\beta = \\frac{(1+\\beta) \\times h \\times c}{\\beta \\times h + c}$\n",
    "    - Donde $\\beta$ es un factor de peso (habitualmente $\\beta = 1$)\n",
    "\n",
    "- **Rango**: Entre 0 y 1, donde 1 indica clustering perfecto\n",
    "- **Interpretación**: Proporciona un balance entre homogeneidad y completitud\n",
    "\n",
    "## Pureza\n",
    "\n",
    "La pureza mide qué tan \"puros\" son los clusters respecto a las clases verdaderas, asignando cada cluster a la clase más frecuente dentro de él.\n",
    "\n",
    "- **Fórmula**: $\\text{Pureza} = \\frac{1}{N} \\sum_{i} \\max_j |c_i \\cap t_j|$\n",
    "    - Donde $c_i$ es el cluster $i$, $t_j$ es la clase $j$, y $N$ es el número total de puntos\n",
    "\n",
    "- **Rango**: Entre 0 y 1, donde 1 indica pureza perfecta\n",
    "- **Ventajas**: Simple e intuitiva\n",
    "- **Desventajas**: No penaliza la fragmentación de una clase en múltiples clusters\n",
    "\n",
    "Estas métricas son especialmente útiles cuando se dispone de etiquetas de clase para validar los resultados del clustering, permitiendo una evaluación cuantitativa de la calidad del agrupamiento desde diferentes perspectivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56abab",
   "metadata": {},
   "source": [
    "# Técnicas para Evaluar Clusters No Etiquetados\n",
    "\n",
    "## Coeficiente de Silhouette\n",
    "\n",
    "El Coeficiente de Silhouette mide qué tan similar es un punto a su propio cluster en comparación con otros clusters, sin necesidad de conocer las etiquetas verdaderas.\n",
    "\n",
    "- **Fórmula**: Para cada punto $i$, se calcula:\n",
    "    $s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$\n",
    "    - Donde $a(i)$ es la distancia media entre $i$ y todos los demás puntos en el mismo cluster\n",
    "    - $b(i)$ es la distancia media mínima entre $i$ y los puntos de cualquier otro cluster\n",
    "\n",
    "- **Rango**: Entre -1 y 1\n",
    "    - Valores cercanos a 1 indican que el punto está bien agrupado\n",
    "    - Valores cercanos a 0 indican que el punto está en el límite entre clusters\n",
    "    - Valores negativos indican que el punto probablemente pertenece a otro cluster\n",
    "\n",
    "- **Interpretación global**: El promedio de todos los coeficientes individuales proporciona una medida de la calidad general del clustering\n",
    "\n",
    "## Índice Calinski-Harabasz\n",
    "\n",
    "También conocido como Criterio de la Razón de Varianza (VRC), este índice evalúa la validez del clustering basándose en la dispersión entre y dentro de los clusters.\n",
    "\n",
    "- **Fórmula**:\n",
    "    $CH = \\frac{SS_B}{SS_W} \\times \\frac{N-k}{k-1}$\n",
    "    - Donde $SS_B$ es la dispersión entre clusters\n",
    "    - $SS_W$ es la dispersión dentro de los clusters\n",
    "    - $N$ es el número total de puntos\n",
    "    - $k$ es el número de clusters\n",
    "\n",
    "- **Interpretación**: Valores más altos indican clusters mejor definidos y más separados\n",
    "- **Ventajas**: Rápido de calcular y eficaz para detectar clusters compactos y bien separados\n",
    "- **Desventajas**: Tiende a favorecer clusters convexos y puede no funcionar bien con formas arbitrarias\n",
    "\n",
    "## Índice Davies-Bouldin\n",
    "\n",
    "Este índice mide la similitud promedio entre cada cluster y su cluster más similar, donde la similitud se define como la ratio entre distancias intra-cluster e inter-cluster.\n",
    "\n",
    "- **Fórmula**:\n",
    "    $DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)$\n",
    "    - Donde $\\sigma_i$ es la distancia media de los puntos en el cluster $i$ a su centroide\n",
    "    - $d(c_i, c_j)$ es la distancia entre los centroides de los clusters $i$ y $j$\n",
    "\n",
    "- **Interpretación**: Valores más bajos indican mejor clustering\n",
    "- **Ventajas**: Tiene en cuenta tanto la dispersión dentro del cluster como la separación entre clusters\n",
    "\n",
    "## Inercia (SSE - Sum of Squared Errors)\n",
    "\n",
    "La inercia mide la compacidad de los clusters calculando la suma de las distancias cuadráticas entre cada punto y el centroide de su cluster.\n",
    "\n",
    "- **Fórmula**:\n",
    "    $\\text{Inercia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$\n",
    "    - Donde $C_i$ es el conjunto de puntos en el cluster $i$\n",
    "    - $\\mu_i$ es el centroide del cluster $i$\n",
    "\n",
    "- **Interpretación**: Valores más bajos indican clusters más compactos\n",
    "- **Aplicación común**: Se utiliza en el método del codo (elbow method) para determinar el número óptimo de clusters\n",
    "\n",
    "Estas métricas proporcionan herramientas objetivas para evaluar la calidad de los agrupamientos cuando no se dispone de etiquetas verdaderas, centrándose en características intrínsecas como compacidad, separación y estructura interna de los clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc092ab",
   "metadata": {},
   "source": [
    "# Clustering en Ciberseguridad\n",
    "\n",
    "El clustering juega un papel fundamental en la ciberseguridad moderna, proporcionando herramientas para identificar patrones, anomalías y comportamientos maliciosos en entornos cada vez más complejos:\n",
    "\n",
    "## Aplicaciones principales\n",
    "\n",
    "- **Detección de anomalías**: Identifica comportamientos atípicos que podrían indicar intrusiones o actividades maliciosas\n",
    "- **Análisis de malware**: Agrupa muestras de malware con características similares para identificar nuevas variantes y familias\n",
    "- **Segmentación del tráfico de red**: Clasifica el tráfico en patrones normales y sospechosos\n",
    "- **Detección de botnets**: Identifica comunicaciones comando y control basadas en patrones de tráfico similares\n",
    "- **Análisis de logs de seguridad**: Agrupa eventos relacionados para identificar incidentes y reducir alertas redundantes\n",
    "- **Prevención de fraude**: Detecta transacciones financieras sospechosas que se desvían de los patrones normales\n",
    "\n",
    "## Algoritmos específicos\n",
    "\n",
    "- **DBSCAN**: Especialmente útil para identificar comportamientos anómalos que se desvían de los grupos principales de actividad normal\n",
    "- **K-means**: Empleado para perfilar comportamientos de usuarios y establecer líneas base de normalidad\n",
    "- **Clustering jerárquico**: Utilizado en análisis forense para establecer relaciones entre artefactos de seguridad\n",
    "- **Modelos de mezclas gaussianas**: Aplicados para modelar distribuciones de comportamiento normal/anormal\n",
    "\n",
    "## Ventajas en ciberseguridad\n",
    "\n",
    "- **Detección no supervisada**: Descubre amenazas sin necesidad de firmas predefinidas o conocimiento previo\n",
    "- **Adaptabilidad**: Ajusta automáticamente sus modelos a medida que evolucionan las amenazas\n",
    "- **Reducción de falsos positivos**: Mejora la precisión de los sistemas de detección\n",
    "- **Escalabilidad**: Maneja grandes volúmenes de datos generados en entornos de red modernos\n",
    "\n",
    "## Desafíos\n",
    "\n",
    "- **Interpretabilidad**: La comprensión de los clusters para generar conocimiento accionable\n",
    "- **Evolución rápida de amenazas**: Necesidad de actualización constante de los modelos\n",
    "- **Alta dimensionalidad**: Los datos de seguridad suelen tener numerosos atributos\n",
    "- **Desequilibrio de clases**: Los eventos maliciosos son generalmente una minoría en los datos\n",
    "\n",
    "El clustering se ha convertido en una técnica esencial dentro de las plataformas avanzadas de seguridad, como SIEM (Security Information and Event Management), EDR (Endpoint Detection and Response) y sistemas de detección de amenazas basados en comportamiento."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
