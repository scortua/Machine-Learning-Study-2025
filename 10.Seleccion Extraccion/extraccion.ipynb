{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631eece5",
   "metadata": {},
   "source": [
    "# PROBLEMA SELECCION DEL MODELO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efc10b",
   "metadata": {},
   "source": [
    "# Selección del Mejor Modelo de Machine Learning\n",
    "\n",
    "La selección del modelo es un paso crítico en el desarrollo de soluciones de machine learning. Es el proceso mediante el cual se elige el algoritmo o modelo más adecuado para resolver un problema específico, basándose en las características de los datos, los requisitos del problema y las métricas de rendimiento.\n",
    "\n",
    "## Factores clave para la selección de modelos\n",
    "\n",
    "1. **Tipo de problema**: Clasificación, regresión, clustering, reducción de dimensionalidad, etc.\n",
    "\n",
    "2. **Características de los datos**: Tamaño del dataset, dimensionalidad, balanceo de clases, ruido, valores faltantes.\n",
    "\n",
    "3. **Complejidad del modelo**: Encontrar el equilibrio adecuado para evitar overfitting (sobreajuste) o underfitting (subajuste).\n",
    "\n",
    "4. **Interpretabilidad**: Algunos problemas requieren modelos interpretables (ej. árboles de decisión) vs. cajas negras (ej. redes neuronales).\n",
    "\n",
    "5. **Recursos computacionales**: Tiempo y memoria disponibles para entrenar y desplegar el modelo.\n",
    "\n",
    "6. **Métricas de rendimiento**: Precisión, recall, F1-score, AUC-ROC, MSE, MAE, etc.\n",
    "\n",
    "## Algoritmos principales\n",
    "\n",
    "### Algoritmos de clasificación\n",
    "- **Regresión Logística**: Simple, interpretable, útil como baseline.\n",
    "- **Árboles de decisión**: Interpretables, manejan bien datos mixtos.\n",
    "- **Random Forest**: Reduce overfitting, maneja bien datos de alta dimensionalidad.\n",
    "- **SVM (Support Vector Machines)**: Efectivo en espacios de alta dimensionalidad.\n",
    "- **KNN (K-Nearest Neighbors)**: Simple, no paramétrico, pero costoso computacionalmente.\n",
    "- **Redes neuronales**: Potentes para problemas complejos, requieren más datos.\n",
    "\n",
    "### Algoritmos de regresión\n",
    "- **Regresión Lineal**: Simple, interpretable, rápida.\n",
    "- **Ridge y Lasso**: Controlan el overfitting mediante regularización.\n",
    "- **Árboles de regresión y Random Forest**: Manejan no linealidades.\n",
    "- **SVR (Support Vector Regression)**: Bueno para problemas no lineales.\n",
    "\n",
    "### Algoritmos de clustering\n",
    "- **K-Means**: Rápido, escalable, pero sensible a outliers.\n",
    "- **DBSCAN**: Detecta clusters de formas arbitrarias, no requiere especificar número de clusters.\n",
    "- **Clustering jerárquico**: Útil para entender relaciones jerárquicas.\n",
    "\n",
    "## Proceso de selección de modelos\n",
    "\n",
    "1. **División de datos**: Train/validation/test sets o cross-validation.\n",
    "2. **Entrenamiento de múltiples modelos**: Probar varios algoritmos con diferentes hiperparámetros.\n",
    "3. **Validación cruzada**: Para evaluaciones más robustas.\n",
    "4. **Evaluación con métricas apropiadas**: Seleccionar métricas relevantes al problema.\n",
    "5. **Optimización de hiperparámetros**: Grid search, random search, Bayesian optimization.\n",
    "6. **Ensamble de modelos**: Combinar múltiples modelos para mejorar el rendimiento.\n",
    "7. **Evaluación final**: Verificar rendimiento en datos de prueba no vistos.\n",
    "\n",
    "La elección del modelo correcto es un balance entre precisión, interpretabilidad, velocidad y complejidad, adaptado a las necesidades específicas del problema que se está resolviendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7c990",
   "metadata": {},
   "source": [
    "# Relevancia y Selección de Características en Machine Learning\n",
    "\n",
    "En el proceso de selección de características, la relevancia de una variable es un concepto fundamental que determina su contribución al modelo predictivo. A continuación se exploran diferentes perspectivas sobre cómo definir la relevancia de una característica:\n",
    "\n",
    "## ¿Cómo se define la relevancia de una característica?\n",
    "\n",
    "1. **Impacto en el rendimiento del modelo**: Una característica se considera relevante si su eliminación deteriora significativamente el rendimiento del clasificador o modelo predictivo. Esta definición está orientada a resultados y evalúa directamente la contribución práctica de la característica.\n",
    "\n",
    "2. **Correlación y dependencia estadística**: La relevancia puede medirse a través de la correlación que presenta una característica con la variable objetivo, así como las interdependencias con otras características. Métodos como la correlación de Pearson, información mutua, o coeficiente de Spearman permiten cuantificar estas relaciones.\n",
    "\n",
    "3. **Relevancia estructural**: Una característica es relevante si forma parte activa de la estructura de decisión del modelo, como ocurre cuando aparece en múltiples nodos de un árbol de decisión o tiene pesos significativos en modelos lineales.\n",
    "\n",
    "4. **Estabilidad de selección**: Las características consistentemente seleccionadas a través de diferentes subconjuntos de datos o diferentes algoritmos de selección suelen considerarse más relevantes y robustas.\n",
    "\n",
    "## Métodos para evaluar la relevancia\n",
    "\n",
    "- **Métodos de filtro**: Evalúan las características independientemente del algoritmo de aprendizaje, utilizando medidas estadísticas como correlación, chi-cuadrado o ganancia de información.\n",
    "  \n",
    "- **Métodos wrapper**: Utilizan el rendimiento del modelo como criterio de evaluación, realizando búsquedas combinatorias como forward selection, backward elimination o recursive feature elimination.\n",
    "\n",
    "- **Métodos embebidos**: Incorporan la selección de características como parte del proceso de entrenamiento del modelo, como ocurre con la regularización Lasso o con los árboles de decisión.\n",
    "\n",
    "La selección efectiva de características no solo mejora el rendimiento predictivo, sino que también incrementa la interpretabilidad del modelo, reduce el costo computacional y ayuda a mitigar el sobreajuste, especialmente en datasets de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc2db6",
   "metadata": {},
   "source": [
    "# Importancia de la Selección de Características\n",
    "\n",
    "## Beneficios fundamentales\n",
    "\n",
    "- **Reducción de dimensionalidad**: Disminuye la complejidad del conjunto de datos, mitigando la \"maldición de la dimensionalidad\" y permitiendo que los algoritmos funcionen de manera más eficiente.\n",
    "\n",
    "- **Mejora del rendimiento**: Los modelos entrenados con características relevantes suelen lograr mayor precisión y capacidad predictiva.\n",
    "\n",
    "- **Optimización computacional**: Reduce significativamente los tiempos de entrenamiento y los requisitos de memoria al procesar menos variables.\n",
    "\n",
    "- **Eliminación de ruido y redundancia**: Filtra características irrelevantes o duplicadas que podrían confundir al algoritmo de aprendizaje.\n",
    "\n",
    "## Impacto en el modelado\n",
    "\n",
    "- **Prevención del sobreajuste**: Al reducir la complejidad del modelo, se minimiza el riesgo de memorizar patrones específicos del conjunto de entrenamiento.\n",
    "\n",
    "- **Mayor generalización**: Los modelos con menos características suelen generalizar mejor a datos nuevos no vistos durante el entrenamiento.\n",
    "\n",
    "- **Interpretabilidad mejorada**: Modelos con menos variables son más comprensibles y facilitan la explicación de las decisiones a stakeholders.\n",
    "\n",
    "- **Visualización efectiva**: Permite representar datos en espacios bidimensionales o tridimensionales, facilitando análisis exploratorios intuitivos.\n",
    "\n",
    "## Aplicaciones prácticas\n",
    "\n",
    "- **Biomedicina**: Identificación de biomarcadores relevantes entre miles de posibles variables genéticas.\n",
    "\n",
    "- **Análisis financiero**: Selección de indicadores económicos con mayor poder predictivo para modelos de inversión.\n",
    "\n",
    "- **Procesamiento de lenguaje**: Reducción de la dimensionalidad del espacio vectorial en análisis de texto.\n",
    "\n",
    "- **Reconocimiento de imágenes**: Extracción de características visuales discriminativas para clasificación eficiente.\n",
    "\n",
    "La selección de características no debe verse como un paso opcional, sino como un componente esencial de un pipeline de machine learning bien diseñado, que impacta directamente en la calidad, eficiencia y aplicabilidad de la solución final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d3f79",
   "metadata": {},
   "source": [
    "# Métodos de Selección de Características\n",
    "\n",
    "## Tratamiento de Valores Perdidos\n",
    "\n",
    "La detección y manejo de características con alta proporción de valores faltantes es crucial para construir modelos robustos:\n",
    "\n",
    "- **Umbral de eliminación**: Se establece un porcentaje crítico (típicamente entre 30-50%) por encima del cual se considera que una característica contiene demasiados valores faltantes para ser útil.\n",
    "- **Implicaciones estadísticas**: Características con muchos datos faltantes pueden introducir sesgos y reducir el poder estadístico del modelo.\n",
    "- **Alternativas a la eliminación**: Antes de descartar estas características, se pueden considerar técnicas de imputación como media/mediana, KNN o modelos predictivos específicos para estimar valores faltantes.\n",
    "\n",
    "## Características con Valores Únicos\n",
    "\n",
    "Las variables que presentan un solo valor único no aportan información discriminativa:\n",
    "\n",
    "- **Varianza cero**: También conocidas como características constantes, no contribuyen a diferenciar entre clases u objetivos.\n",
    "- **Casi-constantes**: Características donde un valor predomina excesivamente (ej. 99% de las observaciones) también pueden considerarse candidatas a eliminación.\n",
    "- **Detección automática**: La mayoría de bibliotecas de preprocesamiento incluyen métodos para identificar y eliminar estas características redundantes.\n",
    "\n",
    "## Manejo de Características Colineales\n",
    "\n",
    "La multicolinealidad entre variables puede afectar negativamente a muchos modelos:\n",
    "\n",
    "- **Correlación de Pearson**: Mide la relación lineal entre variables numéricas, identificando pares con correlación superior al umbral establecido (típicamente 0.7-0.9).\n",
    "- **Matriz de correlación**: Herramienta visual para identificar clusters de variables altamente correlacionadas.\n",
    "- **Estrategias de selección**: Para cada par correlacionado, se puede mantener la variable con mayor correlación con el objetivo o la que tenga mayor importancia según un modelo preliminar.\n",
    "- **Técnicas alternativas**: Para relaciones no lineales, se pueden utilizar correlaciones de Spearman o Kendall, o métodos basados en información mutua.\n",
    "\n",
    "## Características con Cero Importancia\n",
    "\n",
    "Este enfoque aprovecha modelos de machine learning para evaluar la contribución de cada característica:\n",
    "\n",
    "- **Modelos interpretables**: Árboles de decisión y Random Forest proporcionan medidas directas de importancia de características.\n",
    "- **Importancia permutacional**: Evalúa cuánto empeora el rendimiento del modelo cuando los valores de una característica se aleatorizan.\n",
    "- **Proceso iterativo**: Se pueden entrenar modelos preliminares, eliminar características con baja importancia, y reentrenar para verificar que el rendimiento se mantiene o mejora.\n",
    "- **Umbral adaptativo**: En lugar de eliminar características con importancia exactamente cero, se puede establecer un umbral mínimo basado en la distribución de importancias.\n",
    "\n",
    "La combinación de estos métodos en un pipeline de selección de características permite crear modelos más eficientes, interpretables y con mejor capacidad de generalización, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feacdaed",
   "metadata": {},
   "source": [
    "# Métodos de selección: Random Forests\n",
    "\n",
    "## Medición de importancia mediante reducción de impureza\n",
    "\n",
    "Random Forest ofrece un mecanismo natural para evaluar la importancia de características basado en cuánto contribuye cada variable a reducir la impureza a través de los nodos del árbol:\n",
    "\n",
    "- **Impureza de Gini/Entropía**: En problemas de clasificación, una característica importante causará grandes reducciones en la impureza (Gini o entropía) cuando se utiliza para dividir un nodo.\n",
    "- **Reducción de varianza**: En problemas de regresión, las características importantes reducen significativamente la varianza de la variable objetivo en los nodos hijo.\n",
    "- **Importancia acumulada**: La importancia final de una característica se calcula promediando su contribución a la reducción de impureza a través de todos los árboles del bosque.\n",
    "\n",
    "## Ventajas de Random Forest para selección de características\n",
    "\n",
    "- **Robustez ante outliers y ruido**: La naturaleza de ensamble de RF reduce el impacto de datos atípicos en la evaluación de importancia.\n",
    "- **Captura interacciones complejas**: A diferencia de métodos univariados, RF puede detectar la importancia de características que son relevantes solo en combinación con otras.\n",
    "- **No requiere escalado**: Las métricas de importancia son insensibles a la escala de las características, facilitando su interpretación.\n",
    "- **Maneja datos mixtos**: Funciona bien con variables numéricas y categóricas sin transformaciones especiales.\n",
    "\n",
    "## Implementación práctica\n",
    "\n",
    "- **Importancia permutacional**: Método alternativo que mide cuánto empeora el rendimiento del modelo cuando los valores de una característica se aleatorizan, proporcionando una medida más robusta que la importancia basada en impureza.\n",
    "- **Selección recursiva**: Se puede implementar un proceso iterativo donde se eliminan gradualmente las características menos importantes y se reentrenan modelos hasta optimizar el rendimiento.\n",
    "- **Visualización**: Los gráficos de barras de importancia ofrecen una representación intuitiva para identificar rápidamente las características más relevantes.\n",
    "\n",
    "## Consideraciones importantes\n",
    "\n",
    "- **Sesgo hacia variables categóricas**: RF puede sobrevalorar la importancia de características con muchas categorías, requiriendo métodos correctivos.\n",
    "- **Correlación entre características**: Cuando dos variables están altamente correlacionadas, RF puede distribuir la importancia entre ellas, subestimando su relevancia individual.\n",
    "- **Validación cruzada**: Es recomendable calcular la importancia a través de múltiples particiones de datos para asegurar la estabilidad de los resultados.\n",
    "\n",
    "Random Forest no solo ofrece un excelente rendimiento predictivo sino también una herramienta poderosa para la ingeniería y selección de características, proporcionando información valiosa sobre la estructura subyacente de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec786c72",
   "metadata": {},
   "source": [
    "# Extracción de características: Reducción de dimensionalidad\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA (Principal Component Analysis) es una técnica estadística de reducción de dimensionalidad que transforma un conjunto de variables posiblemente correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas componentes principales. Estos componentes se ordenan de manera que los primeros retienen la mayor parte de la variación presente en todas las variables originales.\n",
    "\n",
    "El objetivo principal de PCA es identificar patrones en los datos, expresar los datos de manera que resalten sus similitudes y diferencias, y comprimir los datos reduciendo el número de dimensiones sin perder demasiada información. Al proyectar datos de alta dimensionalidad en un subespacio de menor dimensión, PCA facilita la visualización y análisis de conjuntos de datos complejos.\n",
    "\n",
    "### Funcionamiento de PCA\n",
    "- **Estandarización**: Normaliza los datos para que cada característica tenga media cero y desviación estándar uno\n",
    "- **Cálculo de la matriz de covarianza**: Determina cómo varían las características juntas\n",
    "- **Descomposición en eigenvectores y eigenvalores**: Los eigenvectores representan las direcciones de máxima varianza\n",
    "- **Selección de componentes**: Se seleccionan los primeros k componentes que explican un porcentaje suficiente de la varianza total\n",
    "\n",
    "### Ventajas de PCA\n",
    "- Reduce la dimensionalidad manteniendo la mayor cantidad posible de información\n",
    "- Elimina la multicolinealidad entre variables\n",
    "- Mejora la eficiencia computacional\n",
    "- Facilita la visualización de datos multidimensionales\n",
    "\n",
    "### Limitaciones\n",
    "- Solo captura relaciones lineales entre variables\n",
    "- Puede perder información relevante si las características importantes tienen baja varianza\n",
    "- La interpretabilidad de los componentes puede ser difícil\n",
    "\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "\n",
    "SVD es una técnica de factorización matricial que descompone una matriz en tres matrices separadas, revelando la estructura subyacente de los datos. A diferencia de PCA, que se centra principalmente en la varianza, SVD analiza las relaciones intrínsecas entre filas y columnas de la matriz original.\n",
    "\n",
    "Esta técnica proporciona una representación de rango reducido que captura las características más importantes de los datos, permitiendo aproximar matrices complejas con versiones simplificadas. SVD es particularmente potente porque funciona con cualquier matriz rectangular, no requiere que la matriz sea cuadrada ni definida positiva, y tiene aplicaciones tanto en reducción de dimensionalidad como en sistemas de recomendación, procesamiento de señales y análisis de texto.\n",
    "\n",
    "### Funcionamiento de SVD\n",
    "- **Descomposición**: Factoriza una matriz M en el producto de tres matrices: M = U·Σ·V^T\n",
    "    - U: Matriz ortogonal de vectores singulares izquierdos\n",
    "    - Σ: Matriz diagonal con valores singulares\n",
    "    - V^T: Matriz transpuesta de vectores singulares derechos\n",
    "- **Reducción**: Se conservan solo los k valores singulares más grandes y sus vectores correspondientes\n",
    "\n",
    "### Aplicaciones de SVD\n",
    "- **Sistemas de recomendación**: Utilizado en filtrado colaborativo\n",
    "- **Procesamiento de imágenes**: Compresión y reducción de ruido\n",
    "- **Recuperación de información**: Base para métodos como LSI (Latent Semantic Indexing)\n",
    "- **Pseudoinversa de matrices**: Para resolver sistemas de ecuaciones lineales\n",
    "\n",
    "### Relación con PCA\n",
    "- PCA puede implementarse utilizando SVD\n",
    "- En PCA, se aplica SVD a la matriz de covarianza o directamente a los datos centrados\n",
    "\n",
    "## Comparativa\n",
    "\n",
    "| Aspecto | PCA | SVD |\n",
    "|---------|-----|-----|\n",
    "| Enfoque | Basado en la varianza y covarianza | Factorización directa de matrices |\n",
    "| Aplicabilidad | Mejor para datos densos | Trabaja bien con datos dispersos |\n",
    "| Computación | Puede ser costoso para matrices grandes | Más eficiente para matrices dispersas |\n",
    "| Interpretabilidad | Componentes expresan varianza | Valores singulares expresan importancia |\n",
    "\n",
    "Ambas técnicas son fundamentales en el aprendizaje no supervisado y constituyen la base para muchos otros algoritmos avanzados de reducción de dimensionalidad."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
